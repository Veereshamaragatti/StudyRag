# ğŸ“„ Document Chunking in StudyRAG

## ğŸ¯ Quick Answer

**Chunk Size:** `1000 characters`  
**Overlap:** `200 characters`  
**Algorithm:** Sentence-boundary-aware sliding window

---

## ğŸ“Š What is Chunking?

Chunking is the process of breaking down large documents into smaller, manageable pieces (chunks) for efficient processing and retrieval in the RAG (Retrieval-Augmented Generation) system.

### Why Do We Need Chunking?

1. **Embedding Limitations**: AI models have token limits for generating embeddings
2. **Better Retrieval**: Smaller chunks allow more precise matching with user queries
3. **Context Management**: Chunks provide focused context to the AI model
4. **Memory Efficiency**: Processing smaller pieces reduces memory usage

---

## âš™ï¸ Chunking Configuration

### Default Settings

```typescript
const DEFAULT_CHUNK_SIZE = 1000;  // characters
const DEFAULT_OVERLAP = 200;      // characters
```

### What These Numbers Mean

- **Chunk Size (1000 chars)**: Each chunk contains approximately 1000 characters of text
  - ~150-200 words in English
  - ~2-3 paragraphs
  - Optimal for semantic meaning retention

- **Overlap (200 chars)**: Adjacent chunks share 200 characters
  - ~30-40 words of overlap
  - Prevents information loss at boundaries
  - Maintains context continuity

---

## ğŸ”„ Complete Chunking Process

### Step-by-Step Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DOCUMENT UPLOAD & CHUNKING PROCESS                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ FILE UPLOAD
   â†“
   User uploads PDF/DOCX/TXT file
   â†“
   Multer saves to /uploads directory

2ï¸âƒ£ TEXT EXTRACTION
   â†“
   fileService.parseFile()
   â†“
   â”Œâ”€ PDF: pdf-parse extracts text
   â”œâ”€ DOCX: mammoth extracts text
   â””â”€ TXT: fs.readFile reads text
   â†“
   Raw text extracted

3ï¸âƒ£ TEXT CLEANING
   â†“
   text.replace(/\s+/g, ' ').trim()
   â†“
   - Multiple spaces â†’ single space
   - Remove extra whitespace
   - Trim leading/trailing spaces
   â†“
   Cleaned text ready

4ï¸âƒ£ CHUNKING ALGORITHM
   â†“
   chunkText(text, { chunkSize: 1000, overlap: 200 })
   â†“
   
   For each chunk:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ a. Take 1000 characters from position    â”‚
   â”‚ b. Find last sentence boundary (. ? !)   â”‚
   â”‚ c. Break at boundary if found > 50%      â”‚
   â”‚ d. Otherwise break at 1000 chars         â”‚
   â”‚ e. Move forward by (chunk - overlap)     â”‚
   â”‚ f. Repeat until end of document          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
   Array of text chunks created

5ï¸âƒ£ EMBEDDING GENERATION
   â†“
   embeddingService.generateEmbeddings(chunks)
   â†“
   For each chunk:
   - Send to Gemini text-embedding-004
   - Receive 768-dimensional vector
   â†“
   Array of embeddings created

6ï¸âƒ£ STORAGE
   â†“
   MongoDB documents collection
   â†“
   Store: {
     chunks: [
       { text: "...", embedding: [...] },
       { text: "...", embedding: [...] },
       ...
     ]
   }
   â†“
   âœ… Document ready for retrieval
```

---

## ğŸ’» Technical Implementation

### Core Chunking Function

Located in: `/backend/src/utils/chunkText.ts`

```typescript
export const chunkText = (
  text: string,
  options: ChunkOptions = {}
): string[] => {
  const { chunkSize = 1000, overlap = 200 } = options;

  // Clean text first
  const cleanedText = text.replace(/\s+/g, ' ').trim();

  // If text is smaller than chunk size, return as-is
  if (cleanedText.length <= chunkSize) {
    return [cleanedText];
  }

  const chunks: string[] = [];
  let startIndex = 0;

  while (startIndex < cleanedText.length) {
    const endIndex = startIndex + chunkSize;
    let chunk = cleanedText.slice(startIndex, endIndex);

    // Try to break at sentence boundary
    if (endIndex < cleanedText.length) {
      const lastPeriod = chunk.lastIndexOf('.');
      const lastQuestion = chunk.lastIndexOf('?');
      const lastExclamation = chunk.lastIndexOf('!');
      const lastBreak = Math.max(lastPeriod, lastQuestion, lastExclamation);

      // Break at sentence if found after 50% of chunk
      if (lastBreak > chunkSize * 0.5) {
        chunk = chunk.slice(0, lastBreak + 1);
        startIndex += lastBreak + 1;
      } else {
        startIndex = endIndex;
      }
    } else {
      startIndex = endIndex;
    }

    chunks.push(chunk.trim());

    // Apply overlap for next chunk
    if (startIndex < cleanedText.length && overlap > 0) {
      startIndex = Math.max(0, startIndex - overlap);
    }
  }

  return chunks.filter(chunk => chunk.length > 0);
};
```

---

## ğŸ“ Visual Example

### Example Document Chunking

**Original Text (1500 characters):**
```
Machine learning is a subset of artificial intelligence. 
It focuses on building systems that learn from data. 
These systems improve their performance over time...
[continues for 1500 characters]
```

**After Chunking:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHUNK 1 (1000 chars)                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Machine learning is a subset of artificial             â”‚
â”‚ intelligence. It focuses on building systems that       â”‚
â”‚ learn from data. These systems improve their            â”‚
â”‚ performance over time... [800 more characters]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ 200 chars overlap             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHUNK 2 (1000 chars)                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ...performance over time. [last 200 from chunk 1]       â”‚
â”‚ Neural networks are inspired by biological brains...    â”‚
â”‚ [continues for 800 new characters]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ 200 chars overlap             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHUNK 3 (500 chars - final chunk)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ...biological brains. [last 200 from chunk 2]           â”‚
â”‚ The future of AI looks promising with continued         â”‚
â”‚ advancements. [remaining 300 characters]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Result:** 3 chunks with overlapping content

---

## ğŸ¯ Smart Boundary Detection

### Sentence-Aware Chunking

The algorithm tries to break at natural sentence boundaries:

```typescript
// Find sentence endings
const lastPeriod = chunk.lastIndexOf('.');
const lastQuestion = chunk.lastIndexOf('?');
const lastExclamation = chunk.lastIndexOf('!');
const lastBreak = Math.max(lastPeriod, lastQuestion, lastExclamation);

// Break at sentence if it's after 50% of chunk size
if (lastBreak > chunkSize * 0.5) {
  chunk = chunk.slice(0, lastBreak + 1);
  // ...
}
```

### Why Sentence Boundaries Matter

âœ… **Good Chunking** (at sentence boundary):
```
Chunk 1: "...systems improve performance. The next topic is..."
Chunk 2: "The next topic is neural networks. They consist of..."
```

âŒ **Bad Chunking** (mid-sentence):
```
Chunk 1: "...systems improve performance. The next top"
Chunk 2: "ic is neural networks. They consist of..."
```

---

## ğŸ“Š Chunking Statistics

### Average Chunks per Document Type

| File Type | Average Size | Typical Chunks | Processing Time |
|-----------|-------------|----------------|-----------------|
| TXT (10 pages) | 20 KB | 15-20 chunks | ~2 seconds |
| PDF (10 pages) | 50 KB | 25-35 chunks | ~3 seconds |
| DOCX (10 pages) | 30 KB | 20-30 chunks | ~2.5 seconds |

### Chunk Distribution Example

For a typical 10-page research paper:
```
Total Characters: 30,000
Total Chunks: 30-35 chunks

Chunk Sizes:
- Average: ~950 characters (near 1000 target)
- Minimum: ~600 characters (small paragraphs)
- Maximum: 1000 characters (hard limit)
```

---

## ğŸ” How Chunks Are Used in RAG

### Query â†’ Retrieval â†’ Response Flow

```
1. USER QUERY
   â†“
   "What is machine learning?"
   â†“

2. QUERY EMBEDDING
   â†“
   Generate embedding for query
   768-dim vector: [0.123, -0.456, ...]
   â†“

3. SIMILARITY SEARCH
   â†“
   For each chunk in documents:
   - Calculate cosine similarity
   - Compare query embedding vs chunk embedding
   â†“
   similarity_score = cosine(query_emb, chunk_emb)
   â†“

4. RANK CHUNKS
   â†“
   Chunk 1: 0.89 similarity âœ… (Most relevant)
   Chunk 2: 0.87 similarity âœ…
   Chunk 3: 0.82 similarity âœ…
   Chunk 4: 0.79 similarity âœ…
   Chunk 5: 0.76 similarity âœ…
   Chunk 6: 0.45 similarity âŒ
   â†“

5. SELECT TOP-5 CHUNKS
   â†“
   Combine top 5 chunks as context
   â†“

6. GENERATE ANSWER
   â†“
   Send to Gemini 1.5 Pro:
   - Context: [Combined chunks]
   - Question: "What is machine learning?"
   â†“
   AI generates detailed answer
   â†“

7. RETURN TO USER
   âœ… Answer with relevant context
```

---

## ğŸ¨ Benefits of This Chunking Strategy

### 1. **Context Preservation**
- 200-char overlap ensures no information loss
- Sentences aren't cut mid-thought
- Related concepts stay together

### 2. **Optimal Retrieval**
- 1000-char chunks are small enough for precise matching
- Large enough to contain meaningful context
- Balance between granularity and coherence

### 3. **Efficient Processing**
- Each chunk stays within embedding model limits
- Fast vector search across chunks
- Minimal memory footprint

### 4. **Better AI Responses**
- Top-K retrieval provides focused context
- AI receives relevant information only
- Reduces hallucination by grounding in data

---

## ğŸ”§ Customizing Chunk Settings

### How to Modify Chunk Size

**File:** `/backend/src/services/fileService.ts`

```typescript
// Current settings
const chunks = chunkText(cleanedText, { 
  chunkSize: 1000,  // Change this value
  overlap: 200      // Change this value
});
```

### Recommended Values by Use Case

| Use Case | Chunk Size | Overlap | Reason |
|----------|-----------|---------|---------|
| **Short Docs** | 500 | 100 | Quick answers, less context needed |
| **Standard** â­ | 1000 | 200 | Balanced approach (current) |
| **Long Context** | 1500 | 300 | Complex topics, more context |
| **Technical Docs** | 800 | 150 | Precise code/formulas |

### Trade-offs

**Larger Chunks:**
- âœ… More context per chunk
- âœ… Fewer total chunks
- âŒ Less precise retrieval
- âŒ May exceed token limits

**Smaller Chunks:**
- âœ… More precise matching
- âœ… Faster processing
- âŒ May lose context
- âŒ More chunks to manage

---

## ğŸ“ˆ Performance Considerations

### Processing Time

```
Text Extraction: ~1 second
Chunking: ~0.1 seconds (very fast)
Embedding Generation: ~2-5 seconds (API call)
MongoDB Storage: ~0.5 seconds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~3-7 seconds per document
```

### Memory Usage

```
10 MB document â†’ ~8,000 chunks (average)
Each chunk â†’ 768-dim embedding (3 KB)
Total memory: ~24 MB for embeddings
```

### Optimization Tips

1. **Batch Processing**: Generate embeddings for multiple chunks at once
2. **Caching**: Cache embeddings to avoid regeneration
3. **Async Operations**: Process chunks in parallel when possible
4. **Cleanup**: Remove old document chunks when updating

---

## ğŸ› Troubleshooting

### Common Issues

**Issue 1: Chunks are too small**
```
Symptom: Chunks < 500 characters consistently
Cause: Document has short paragraphs
Solution: Reduce overlap or increase chunk size
```

**Issue 2: Context lost between chunks**
```
Symptom: AI gives incomplete answers
Cause: Overlap too small
Solution: Increase overlap to 300-400 characters
```

**Issue 3: Too many chunks generated**
```
Symptom: Slow retrieval, high storage
Cause: Chunk size too small
Solution: Increase chunk size to 1200-1500
```

**Issue 4: Chunks break mid-sentence**
```
Symptom: Awkward chunk boundaries
Cause: No sentence endings found in range
Solution: Already handled by 50% threshold check
```

---

## ğŸ”¬ Code Locations

### Key Files

1. **Chunking Logic**
   - Path: `/backend/src/utils/chunkText.ts`
   - Function: `chunkText()`
   - Interface: `ChunkOptions`

2. **File Processing**
   - Path: `/backend/src/services/fileService.ts`
   - Function: `parseFile()`
   - Calls chunking after text extraction

3. **Document Controller**
   - Path: `/backend/src/controllers/documentController.ts`
   - Function: `uploadDocument()`
   - Orchestrates the full upload + chunking flow

4. **Embedding Service**
   - Path: `/backend/src/services/embeddingService.ts`
   - Function: `generateEmbeddings()`
   - Generates vectors for chunks

---

## ğŸ“ Summary

### Key Takeaways

âœ… **Chunk Size**: 1000 characters (optimal balance)  
âœ… **Overlap**: 200 characters (preserves context)  
âœ… **Algorithm**: Sentence-boundary-aware sliding window  
âœ… **Process**: Extract â†’ Clean â†’ Chunk â†’ Embed â†’ Store  
âœ… **Benefits**: Better retrieval, preserved context, efficient processing  

### The Chunking Formula

```
Number of Chunks = ceil((Text Length - Overlap) / (Chunk Size - Overlap))

Example:
- Text: 10,000 characters
- Chunk Size: 1000
- Overlap: 200

Chunks = ceil((10,000 - 200) / (1000 - 200))
       = ceil(9,800 / 800)
       = 13 chunks
```

---

## ğŸ“š Further Reading

- **RAG Architecture**: See `ARCHITECTURE.md` for complete system flow
- **Embedding Service**: See embedding generation process
- **Vector Search**: Understand how chunks are retrieved
- **Gemini API**: Learn about text-embedding-004 model

---

**Need help?** Check `TROUBLESHOOTING.md` for common issues!

---

*This document explains the chunking mechanism used in StudyRAG's RAG system.*
*Last Updated: 2026-01-21*
